# Micrograd: My Autograd Engine in Python ðŸš€

This project is a **minimalistic implementation of an automatic differentiation engine**, inspired by [Andrej Karpathyâ€™s Micrograd](https://github.com/karpathy/micrograd).  
I built it from scratch to deeply understand how **backpropagation** and **computational graphs** work.


---

## ðŸ“š Features
- Pure Python implementation (no external ML libraries)
- Reverse-mode autodiff (like PyTorch)
- Support for fully connected neural networks
- Introspectable computation graph
- Backward pass with gradient tracking

---

## ðŸ§  Motivation
This repo is a hands-on learning tool to:
- Understand computational graphs  
- See how autograd engines work under the hood  
- Learn by building, not just using libraries
