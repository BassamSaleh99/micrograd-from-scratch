# Micrograd: My Autograd Engine in Python ðŸš€

This project is a minimalistic implementation of an automatic differentiation engine, inspired by [Andrej Karpathy's Micrograd](https://github.com/karpathy/micrograd).

I built this project from scratch to deeply understand how backpropagation and computational graphs work. It supports scalar-based reverse-mode autodiff and includes a simple neural network example.

---

## ðŸ“š Features

- Pure Python implementation
- Reverse-mode autodiff (like PyTorch)
- Fully connected neural network support
- Introspectable computation graph
- Backward pass with gradient tracking

---

ðŸ§  Motivation
This repo is a hands-on learning tool to:

Understand computational graphs

See how autograd engines work under the hood

Learn by building, not just using libraries